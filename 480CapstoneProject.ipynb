{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd435b7b-dc4c-432d-ad6f-f8911845290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "import numpy as np\n",
    "\n",
    "#data = np.load(\"C:/Users/calvi/Desktop/School/ANSC480/DataCode/results_dataset/2019-11-27--10_24_59/000200/behaviour_25.npy\", allow_pickle=True)\n",
    "data_november_29 = []\n",
    "for i in range(200, 315):\n",
    "    data = np.load(\"C:/Users/calvi/Desktop/School/ANSC480/DataCode/results_dataset/2019-11-27--10_24_59/000\" + str(i) + \"/behaviour_15.npy\", allow_pickle=True)\n",
    "    data_november_29.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "243e4f3f-1e7b-4c9a-8b77-a91d46efe9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "def load_frames(video_index):\n",
    "    video = cv2.VideoCapture(\"C:/Users/calvi/Desktop/School/ANSC480/DataCode/PIGS291119/000\" + str(video_index) + \"/color.mp4\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "\n",
    "    while video.isOpened():\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    video.release()\n",
    "    return frames\n",
    "\n",
    "def load_times(video_index):\n",
    "    times = []\n",
    "    with open(\"C:/Users/calvi/Desktop/School/ANSC480/DataCode/PIGS291119/000\" + str(video_index) + \"/times.txt\", \"r\") as file:\n",
    "        for line in file:\n",
    "            times.append(get_time(line.strip()))\n",
    "    return times\n",
    "\n",
    "def is_eating(i):\n",
    "    if i == 4:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def at_feeder(i):\n",
    "    return i == 1\n",
    "\n",
    "def display_frame(frame_data, frame):\n",
    "    frame_copy = frame.copy()\n",
    "    i = 9\n",
    "    x1 = int(frame_data[1])\n",
    "    y1 = int(frame_data[2])\n",
    "    x2 = int(frame_data[3])\n",
    "    y2 = int(frame_data[4])\n",
    "    #print(data[i][1][0])\n",
    "    cv2.rectangle(frame_copy, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "    frame_copy_rgb = cv2.cvtColor(frame_copy, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imshow('Frame', frame_copy)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def get_time(timestamp):\n",
    "    return datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S.%f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7ed3f6c9-dc97-4989-8805-52710ef39eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Define the paths to the annotation and image files\n",
    "annotation_file = 'C:/Users/calvi/Desktop/School/ANSC480/DataCode/dataset/labels/train/000025.txt'  # YOLO annotation file\n",
    "image_file = 'C:/Users/calvi/Desktop/School/ANSC480/DataCode/dataset/images/train/000025.jpg'  # Corresponding image file\n",
    "\n",
    "# Load the image using OpenCV\n",
    "image = cv2.imread(image_file)\n",
    "\n",
    "# Get the image dimensions\n",
    "img_height, img_width, _ = image.shape\n",
    "\n",
    "# Read the annotation file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = f.readlines()\n",
    "\n",
    "# Loop through each annotation and draw the corresponding bounding box\n",
    "for annotation in annotations:\n",
    "    # Split the annotation into components\n",
    "    parts = annotation.strip().split()\n",
    "    class_id = int(parts[0])  # Class ID\n",
    "    x_center = float(parts[1])  # Normalized x-center\n",
    "    y_center = float(parts[2])  # Normalized y-center\n",
    "    width = float(parts[3])  # Normalized width\n",
    "    height = float(parts[4])  # Normalized height\n",
    "    \n",
    "    # Convert normalized coordinates to pixel coordinates\n",
    "    x_center = int(x_center * img_width)\n",
    "    y_center = int(y_center * img_height)\n",
    "    width = int(width * img_width)\n",
    "    height = int(height * img_height)\n",
    "\n",
    "    # Calculate the top-left and bottom-right corners of the bounding box\n",
    "    x1 = x_center - width // 2\n",
    "    y1 = y_center - height // 2\n",
    "    x2 = x_center + width // 2\n",
    "    y2 = y_center + height // 2\n",
    "\n",
    "    # Draw the bounding box on the image\n",
    "    color = (0, 255, 0)  # Green color for the bounding box\n",
    "    thickness = 2  # Line thickness\n",
    "    image = cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)\n",
    "    # Optionally, put the class ID text near the bounding box\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    image = cv2.putText(image, str(class_id), (x1, y1 - 10), font, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image with bounding boxes\n",
    "cv2.imshow('Image with Bounding Boxes', image)\n",
    "\n",
    "# Wait for a key press and close the image window\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ae450f1-cc74-4ed5-8293-a6c933e59382",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "image_file = 'C:/Users/calvi/Desktop/School/ANSC480/DataCode/dataset/images/train/000025.jpg'\n",
    "# Load the annotations\n",
    "with open(\"C:/Users/calvi/Desktop/School/ANSC480/DataCode/annotated/2019_11_28/000113/output.json\", \"r\") as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Select the frame numbers you want to inspect manually\n",
    "frames_to_check = [5, 10, 15]  # Example frame numbers\n",
    "\n",
    "# Define the image extension (e.g., .jpg)\n",
    "image_extension = \".jpg\"\n",
    "image = Image.open(image_file)\n",
    "# Function to display image with bounding boxes\n",
    "def display_image_with_bbox(image_path, bbox):\n",
    "    \n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    x, y, w, h = bbox[\"x\"], bbox[\"y\"], bbox[\"width\"], bbox[\"height\"]\n",
    "    draw.rectangle([x, y, x + w, y + h], outline=\"red\", width=2)\n",
    "\n",
    "      # This will open the image in the default viewer\n",
    "\n",
    "# Loop through the selected frames\n",
    "  \n",
    "for pig in annotations[\"objects\"]:\n",
    "    last_fnum_bbox = 0\n",
    "    for frame in pig[\"frames\"]:\n",
    "        fnum = frame[\"frameNumber\"]\n",
    "        if fnum > 25:\n",
    "            break\n",
    "        else:\n",
    "            last_fnum_bbox = frame[\"bbox\"]        \n",
    "    display_image_with_bbox(image_path, last_fnum_bbox)\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "355439aa-85fc-4b0c-9bd4-3f1b6d3319c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4 0.426172 0.425694 0.210156 0.215278', '5 0.635938 0.718056 0.140625 0.255556', '0 0.865234 0.499306 0.111719 0.234722', '1 0.640625 0.332639 0.150000 0.154167', '2 0.497656 0.258333 0.193750 0.188889', '3 0.261328 0.792361 0.252344 0.404167', '6 0.229687 0.588889 0.182812 0.294444', '7 0.648828 0.684722 0.167969 0.291667']\n"
     ]
    }
   ],
   "source": [
    "print(frame_annotations[25])\n",
    "framelists[\"objects\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa60293-e199-42d6-ad65-f94a75a01e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames: 1800\n",
      "✅ Labels converted and saved.\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "#Extract frame annotation data for YOLO training\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Replace with your actual JSON file path\n",
    "#json_file = \"C:/Users/calvi/Desktop/School/ANSC480/DataCode/annotated/2019_11_28/000113/output.json\"\n",
    "json_file = \"C:/Users/calvi/Desktop/School/ANSC480/DataCode/annotated/2019_12_02/000005/output.json\"\n",
    "with open(json_file, \"r\") as f:\n",
    "    framelists = json.load(f)\n",
    "\n",
    "#video_path = \"C:/Users/calvi/Desktop/School/ANSC480/DataCode/annotated/2019_11_28/000113/color.mp4\"  # Replace with your actual path\n",
    "video_path = \"C:/Users/calvi/Desktop/School/ANSC480/DataCode/annotated/2019_12_02/000005/color.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "max_frame_count = 0\n",
    "fnum_offset = 1800 #For loading in different days, you'll need to adjust the names of the files to avoid overlap\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    max_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"Total number of frames:\", max_frame_count)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Image dimensions (required for normalization)\n",
    "IMG_WIDTH = 1280  # adjust to match your dataset\n",
    "IMG_HEIGHT = 720\n",
    "\n",
    "# Output directory for YOLO label files\n",
    "output_dir_train = \"dataset/labels/train\"\n",
    "output_dir_test = \"dataset/labels/test\"\n",
    "output_dir_val = \"dataset/labels/val\"\n",
    "#os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def convert_to_yolo_bbox(x, y, w, h):\n",
    "    x_center = (x + w / 2) / IMG_WIDTH\n",
    "    y_center = (y + h / 2) / IMG_HEIGHT\n",
    "    width = w / IMG_WIDTH\n",
    "    height = h / IMG_HEIGHT\n",
    "    return x_center, y_center, width, height\n",
    "\n",
    "# Group annotations by frameNumber\n",
    "frame_annotations = {}\n",
    "last_seen = {}\n",
    "\n",
    "for pig in framelists[\"objects\"]:\n",
    "    pig_id = int(pig[\"id\"])  # class label\n",
    "    dict_by_frame = {\n",
    "        item[\"frameNumber\"]: {k: v for k, v in item.items() if k != \"frame_number\"}\n",
    "        for item in pig[\"frames\"]\n",
    "        }\n",
    "    for fnum in range(0, max_frame_count):\n",
    "        \n",
    "        #if not frame.get(\"visible\", False):\n",
    "        #    continue\n",
    "        #fnum = frame[\"frameNumber\"]\n",
    "        if fnum in dict_by_frame.keys():\n",
    "            bbox = dict_by_frame[fnum][\"bbox\"]\n",
    "            x, y, w, h = bbox[\"x\"], bbox[\"y\"], bbox[\"width\"], bbox[\"height\"]\n",
    "            x_c, y_c, bw, bh = convert_to_yolo_bbox(x, y, w, h)\n",
    "            label = f\"{pig_id} {x_c:.6f} {y_c:.6f} {bw:.6f} {bh:.6f}\"\n",
    "            frame_annotations.setdefault(fnum, []).append(label)\n",
    "            \n",
    "            last_seen[pig_id] = label\n",
    "        else:\n",
    "            frame_annotations.setdefault(fnum, []).append(last_seen[pig_id])\n",
    "\n",
    "#all_frame_nums = sorted(frame_annotations.keys())\n",
    "#for fnum in all_frame_nums:\n",
    "#    present_ids = {int(lbl.split()[0]) for lbl in frame_annotations[fnum]}\n",
    "#    for pig_id in range(8):  # Ensure pigs 0–7\n",
    "#        if pig_id not in present_ids and pig_id in last_seen:\n",
    "            # Use the last seen bbox if pig missing\n",
    "#            frame_annotations[fnum].append(last_seen[pig_id])\n",
    "\n",
    "# Save .txt file for each frame\n",
    "for fnum, labels in frame_annotations.items():\n",
    "    label_path = os.path.join(output_dir_train, f\"{fnum+fnum_offset:06}.txt\")\n",
    "    if fnum > len(frame_annotations.items()) * 0.9:\n",
    "        label_path = os.path.join(output_dir_val, f\"{fnum+fnum_offset:06}.txt\")\n",
    "    elif fnum > len(frame_annotations.items()) * 0.7:\n",
    "        label_path = os.path.join(output_dir_test, f\"{fnum+fnum_offset:06}.txt\")\n",
    "    with open(label_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(labels))\n",
    "\n",
    "print(\"✅ Labels converted and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c746e3-b2a5-408c-95b8-054eec40bf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 1800 frames\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "#Transfer Frame data for Yolo training\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Path to your input video\n",
    "output_image_dir_train = \"dataset/images/train\"\n",
    "output_image_dir_test = \"dataset/images/test\"\n",
    "output_image_dir_val = \"dataset/images/val\"\n",
    "\n",
    "#os.makedirs(output_image_dir, exist_ok=True)\n",
    "\n",
    "# Only extract frames that appear in annotation file\n",
    "target_frames = set(frame_annotations.keys())  # from previous step\n",
    "\n",
    "# Open the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_idx = 0\n",
    "saved = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if frame_idx in target_frames:\n",
    "        filename = f\"{frame_idx+fnum_offset:06}.jpg\"\n",
    "        out_path = os.path.join(output_image_dir_train, filename)\n",
    "        if frame_idx > len(target_frames) * 0.9:\n",
    "            out_path = os.path.join(output_image_dir_val, filename)\n",
    "        elif frame_idx > len(target_frames) * 0.7:\n",
    "            out_path = os.path.join(output_image_dir_test, filename)\n",
    "        \n",
    "        cv2.imwrite(out_path, frame)\n",
    "        saved += 1\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()\n",
    "print(f\"✅ Saved {saved} frames\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f0a23b-4b83-41ca-aeda-a45d7ee56162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e1d027-c4ad-41c9-813b-d89fd5a2cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "#Need to train model to identify each of the 8 pigs at each frame\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO model (choose a pre-trained version, e.g., YOLOv5s)\n",
    "yolo_model = YOLO(\"yolo11n.pt\")  # you can choose other pre-trained weights like yolov5m.pt, yolov5l.pt, etc.\n",
    "\n",
    "# Train the model using the data\n",
    "yolo_model.train(data='C:/Users/calvi/Desktop/School/ANSC480/DataCode/dataset/datasets.yaml', epochs=200, imgsz=640, batch=16)\n",
    "val_results = yolo_model.val()\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87db465-0b9a-4de0-a91e-ef9088c0993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract key metrics\n",
    "val_results = results\n",
    "metrics = val_results.results_dict  # This gives a dictionary of metrics\n",
    "# Print to see what's inside\n",
    "print(metrics)\n",
    "# Separate metrics into keys and values\n",
    "keys = list(metrics.keys())\n",
    "values = list(metrics.values())\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(keys, values, color='skyblue')\n",
    "plt.xlabel(\"Value\")\n",
    "plt.title(\"YOLO Validation Metrics\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ee299-a924-4dbb-afc6-0e12f6b9b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "import math\n",
    "#imperfect function for estimating what pig is currently the focus of the current frame data\n",
    "def which_pig(frame, frame_data, prediction):\n",
    "    center_x = (frame_data[3] - frame_data[1]) / 2\n",
    "    center_y = (frame_data[4] - frame_data[2]) / 2\n",
    "    best_center_dist = math.inf\n",
    "    best_center_class = -1\n",
    "    for box in prediction.boxes:\n",
    "        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "        conf = box.conf[0].item()\n",
    "        cls = int(box.cls[0].item())\n",
    "        pred_center_x = (x2-x1)/2\n",
    "        pred_center_y = (y2-y1)/2\n",
    "        if conf > 0.8:\n",
    "            if math.dist((pred_center_x, pred_center_y), (center_x, center_y)) < best_center_dist and math.dist((pred_center_x, pred_center_y), (center_x, center_y)) < 200:\n",
    "                best_center_dist = math.dist((pred_center_x, pred_center_y), (center_x, center_y)) \n",
    "                best_center_class = cls\n",
    "    return best_center_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dcae4acd-4d98-434a-ad12-82349fea1afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 pig_3, 38.8ms\n",
      "Speed: 4.3ms preprocess, 38.8ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "frames = load_frames(200+5)\n",
    "frame = frames[1].copy()\n",
    "results = yolo_model(frame)\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82004221-a05e-408a-add9-d594c2cdf175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "pigs_feeding_times = [[]] * 8 \n",
    "#pig0_feeding_times = []\n",
    "for i in range(0, len(data_november_29)): #len(data_november_29) or 50\n",
    "    frames = load_frames(200 + i)\n",
    "    prediction = yolo_model(frames[0])[0]\n",
    "    for tracklet in range(0, data_november_29[i].shape[0]):\n",
    "        if data_november_29[i].size == 0:\n",
    "            continue\n",
    "        pig_data = data_november_29[i][tracklet]\n",
    "        pig_times = load_times(200 + i) #Clips start at 200 for November 29th\n",
    "        \n",
    "        pig_id = which_pig(frames[0], pig_data[0], prediction) #All frames in a tracklet should be of one pig\n",
    "        for frame_data in pig_data:\n",
    "            if is_eating(frame_data[13]) and at_feeder(frame_data[12]):\n",
    "                \n",
    "                if pig_id == -1:\n",
    "                    pigs_feeding_times[tracklet % 8].append(pig_times[int(frame_data[0])])\n",
    "                else:\n",
    "                    pigs_feeding_times[pig_id].append(pig_times[int(frame_data[0])])\n",
    "                #pig0_feeding_times.append(pig0_times[int(frame_data[0])])\n",
    "                \n",
    "                #display_frame(frame_data, frames[int(frame_data[0])])\n",
    "                #pigs_feeding_times[which_pig(frame, frame_data, model)] = (pig0_times[int(frame_data[0])], duration)\n",
    "                #break #Commenting this out will probably result in too much memory being used, recording every single frame that has feeding behavior\n",
    "                \n",
    "                #break\n",
    "    del frames\n",
    "pigs_feeding_times = [sorted(times) for times in pigs_feeding_times] #All pigs feeding times, only one instance max per 5 minute video\n",
    "\n",
    "#for pig in range(0, len(pigs)):\n",
    "#    pigs_feeding_times[pig] = sorted(pigs_feeding_times[pig], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b1df05c2-1843-463d-ba31-ad3f59fc6750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pigs_feeding_times = sorted(pigs_feeding_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d3465-2797-4b55-81aa-7f6280c2805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_tensor)\n",
    "print(y_train_tensor)\n",
    "print(X_test_tensor)\n",
    "print(y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad714fbc-7821-495b-9738-164d492cfa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "#for i in range(1, 10):\n",
    "#    with torch.no_grad():\n",
    "        #X_tensor = torch.tensor(intervals[-(SEQ_LENGTH+i):-i]).unsqueeze(0).unsqueeze(-1)\n",
    "        #test_input = torch.tensor(intervals[-(SEQ_LENGTH+i):-i]).unsqueeze(0).unsqueeze(-1)\n",
    "predicted = model(X_test_tensor)\n",
    "        #actual = intervals[i] - intervals[i-1]\n",
    "print(predicted)\n",
    "print(y_test_tensor)\n",
    "\n",
    "predicted = model(X_train_tensor)\n",
    "print(predicted)\n",
    "print(y_train_tensor)\n",
    "        #print(f\"/n⏱️ Predicted next feeding interval: {predicted:.2f} minutes\")\n",
    "        #print(f\"/n⏱️ Actual next feeding interval: {actual:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798c68a-c257-40ba-8850-0a502683a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.5\n",
    "def condense_feeding(feeding_times):\n",
    "    feeding_durations = []\n",
    "    start_time = 0\n",
    "    prev_time = 0\n",
    "    for time in feeding_times:\n",
    "        if time - prev_time < 0.15: #9 seconds\n",
    "            prev_time = time\n",
    "        else:\n",
    "            feeding_durations.append((start_time, prev_time - start_time))\n",
    "            #feeding_durations = np.append(feeding_durations, (start_time, prev_time - start_time))\n",
    "            start_time = time\n",
    "            prev_time = time\n",
    "    feeding_durations.append((start_time, prev_time - start_time))\n",
    "    #feeding_durations = np.append(feeding_durations, (start_time, prev_time - start_time))\n",
    "    return feeding_durations\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f47b3d6-cf7e-41dc-ab2d-dafb62cb150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(feeding_times)\n",
    "#len(feeding_times[0])\n",
    "#print(feeding_times)\n",
    "#print(pigs_feeding_times)\n",
    "len(durations)\n",
    "durations[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b72307-7ba8-46aa-95f7-ed3032296ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "#Pretty much the RFID tag feeding prediction\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "#feeding_times = [datetime.fromisoformat(t) for t in feeding_times]\n",
    "\n",
    "\n",
    "#intervals = np.diff([t.timestamp() for t in feeding_times]) / 60  # minutes\n",
    "#intervals = np.array(intervals, dtype=np.float32)\n",
    "#intervals = np.array([t.timestamp()-feeding_times[:][0].timestamp() for t in feeding_times[:]], dtype=np.float32) / 60\n",
    "#print(\"intervals\")\n",
    "#print(intervals)\n",
    "\n",
    "feeding_times = pigs_feeding_times[0]  # Choose pig 0 or any specific pig\n",
    "intervals = np.array([t.timestamp() - feeding_times[0].timestamp() for t in feeding_times], dtype=np.float32) / 60\n",
    "\n",
    "\n",
    "durations = condense_feeding(intervals)\n",
    "\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x_seq = data[i:i+seq_length] # each x is a sequence of (start_time, duration)\n",
    "        x_seq[:][0] *= -1\n",
    "        x_seq[:][0] += data[i + seq_length-1][0]\n",
    "        next_start_time = data[i + seq_length][0]\n",
    "        prev_start_time = data[i + seq_length - 1][0]\n",
    "        delta = next_start_time - prev_start_time\n",
    "        ys.append(delta)\n",
    "\n",
    "        xs.append(x_seq)\n",
    "    return np.array(xs, dtype=np.float32), np.array(ys, dtype=np.float32)\n",
    "\n",
    "SEQ_LENGTH = 4\n",
    "X, y = create_sequences(durations, SEQ_LENGTH)\n",
    "\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "\n",
    "#X_train_tensor = torch.tensor(X_train).unsqueeze(-1)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train).unsqueeze(-1)\n",
    "#X_test_tensor = torch.tensor(X_test).unsqueeze(-1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_test_tensor = torch.tensor(y_test).unsqueeze(-1)\n",
    "#X_tensor = torch.tensor(X).unsqueeze(-1)  # shape: (batch, seq, 1)\n",
    "#y_tensor = torch.tensor(y).unsqueeze(-1)\n",
    "\n",
    "\n",
    "class FeedLSTM(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=32):\n",
    "        super(FeedLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]           \n",
    "        out = self.relu(self.fc1(out))  \n",
    "        out = self.fc2(out)     \n",
    "        return out\n",
    "\n",
    "model = FeedLSTM()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "\n",
    "EPOCHS = 500\n",
    "TestLoss_tolerance = 0.05\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    output = model(X_train_tensor)\n",
    "    loss = loss_fn(output, y_train_tensor)\n",
    "    #print(output, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_output = model(X_test_tensor)\n",
    "            test_loss = loss_fn(test_output, y_test_tensor)\n",
    "        print(f\"Epoch {epoch}: Train Loss = {loss.item():.4f}, Test Loss = {test_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    recent_sequence = np.array(durations[-SEQ_LENGTH:], dtype=np.float32)\n",
    "    test_input = torch.tensor([recent_sequence])  # shape: (1, SEQ_LENGTH, 2)\n",
    "    predicted_start_time = model(test_input).item()\n",
    "    actual_start_time = durations[-1][0] - durations[-2][0]  # last known start time\n",
    "\n",
    "    print(f\"\\n⏱️ Predicted next feeding start time: {predicted_start_time:.2f} minutes since start\")\n",
    "    print(f\"⏱️ Actual next feeding time: {actual_start_time:.2f} minutes since start\")\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f0de6-11d3-4c99-b9d1-9fd3b80d2040",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for offset in range(0, 10):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        recent_sequence = np.array(durations[-SEQ_LENGTH-offset:-1-offset], dtype=np.float32)\n",
    "        test_input = torch.tensor([recent_sequence])  # shape: (1, SEQ_LENGTH, 2)\n",
    "        predicted_start_time = model(test_input).item()\n",
    "        actual_start_time = durations[-offset-1][0] - durations[-offset-2][0]  # last known start time\n",
    "    \n",
    "        print(f\"\\n⏱️ Predicted next feeding start time: {predicted_start_time:.2f} minutes since start\")\n",
    "        print(f\"⏱️ Actual next feeding time: {actual_start_time:.2f} minutes since start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b41d7-3815-4f3c-84fb-f8925666d021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
